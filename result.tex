\label{chapter:results}
We evaluated the \toolname\ testing approach by conducting an empirical study on a smaller dataset of apps \ie 60 \textit{APKs}.
Afterwards, we used the collected crash logs for evaluating the correctness and reliability of our clustering approach, forming a bucket of unique crash reports. 
Finally, we chose the 3 apps that seemed most relevant to our study with enough reports generated by the testing tools and suitable reviews for investigating the accuracy of our linking approach. 

\section{Testing experiment with \monkey and \sapienz}
To conduct our experiment, we selected a dataset of 60 \textit{APKs}, grouping them together exploiting the FDroid-crawler we built. 
We made our selection as follows: we tried to chose a dataset containing the most varied number of \textit{APKs}. Indeed, we chose very popular apps which have a high number of downloads (\eg \textit{Telegram}) as well as less known apps with a low number of downloads (\eg \textit{Ringdroid}). 
The entire dataset can be found in the folder "Dataset" in the \toolname\ source code.  
Afterwards, we tested the whole dataset 3 times with the Android testing tools, i.e., \monkey and \sapienz, running each tool for 30 minutes per app.
According to the testing cycles described in the section \ref{approach:testing}, this can be translated as follows: 
\begin{enumerate}
\item Number of iteration characterizing the \textit{session cycle}: \textbf{3 iterations}; 
\item Number of apps forming the \textit{dataset cycle}: \textbf{60 apps}; 
\item Time frame describing the \textit{single app cycle}: \textbf{30 min}.
\end{enumerate}
We conducted our experiment in the following environment: 2 Samsung Galaxy Tab 8 inches, with Android Kitkat 4.4 (API 19) on which we run the automated testing tools and Mac OS 10.11 for starting \toolname.
The testing parameters we inserted in the settings file are summarized in the table \ref{tbl: chosenparameters}. 
\begin{table}[tb]
\centering
\caption{Chosen parameters for \sapienz and \monkey to conduct our empirical study}
\label{tbl: chosenparameters}
\begin{tabular}{l|l|}
\hline
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}\textbf{Automated testing tool}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{Chosen parameters}} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{FFFFFF}\textit{\textbf{Monkey}}}        & \textit{verbosity = -v -v -v}                                           \\ \hline
                                                                              & \textit{random events = 3000}                                           \\ \cline{2-2} 
                                                                              & \textit{delay between events = 10}                                      \\ \cline{2-2} 
                                                                              & \textit{percentage touch events = 8}                                    \\ \cline{2-2} 
                                                                              & \textit{percentage system events = 8}                                   \\ \cline{2-2} 
                                                                              & \textit{percentage motion events = 8}                                   \\ \cline{2-2} 
                                                                              & \textit{ignore crashes = true}                                          \\ \hline
\multicolumn{1}{|c|}{\textit{\textbf{Sapienz}}}                               & \textit{min sequence = 20}                                              \\ \hline
                                                                              & \textit{max sequence = 500}                                             \\ \cline{2-2} 
                                                                              & \textit{suite size = 5}                                                 \\ \cline{2-2} 
                                                                              & \textit{population size = 50}                                           \\ \cline{2-2} 
                                                                              & \textit{offspring size = 50}                                                     \\ \cline{2-2} 
                                                                              & \textit{generation = 100}                                               \\ \cline{2-2} 
                                                                              & \textit{crossover = 0.7}                                                \\ \cline{2-2} 
                                                                              & \textit{mutation = 0.3}                                                 \\ \cline{2-2} 
\end{tabular}
\end{table}
	
The reason behind the parameters we chosen for \monkey is based on experimental results. Indeed, we conducted few demo-experiments in order to state which combination of parameters led to the greater number of crashes. 
Furthermore, we changed the percentage of the system, motion and touch events at the beginning of each \textit{dataset cycle} in order to variate our testing strategy. 
The selection behind the \sapienz parameters is consistent with the empirical study conducted by Mao \etal \cite{sapienz}.  
At the end of each {session cycle} we used our clustering approach to deduplicate the collected crash logs and form a bucket of unique ones. 


Table \ref{tbl: results} shows the results of the empirical study we conducted. 
As shown in the table, \sapienz revealed the largest number of unique crash logs in each iteration of the experiment. 
However, it should be noted that the number of total crashes revealed by \monkey in the third dataset cycle is clear greater that the ones found by \sapienz. 
This because, the percentage of \textit{system} events has conspicuously grown (from 8\% to 35\%). This caused some sequences of events of that type that could not be elaborated by the devices, leading the app to failure. 
In fact, the number of unique crashes for \monkey is not greater than the ones found by \sapienz, since they do not represent crashes caused by the application under test, but they can be categorized as native crashes. 

\begin{table}[tb]
\centering
\caption{Testing results}
\label{tbl: results}
\begin{tabular}{llcc|}
\hline
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{|l}{\cellcolor[HTML]{EFEFEF}\textit{\textbf{Iteration number}}} & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}\textbf{Crashes}} & \multicolumn{1}{l}{\cellcolor[HTML]{EFEFEF}\textit{\textbf{Monkey}}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}\textit{\textbf{Sapienz}}} \\ \hline
\multicolumn{1}{|l|}{\textit{\textbf{First dataset cycle}}}                     & \multicolumn{1}{l|}{App crashed}                             & 7                                                                    & 16                                                                     \\ \cline{1-1}
\multicolumn{1}{l|}{}                                                           & \multicolumn{1}{l|}{Unique crashes}                          & 14                                                                   & 19                                                                     \\
\multicolumn{1}{l|}{}                                                           & \multicolumn{1}{l|}{Total crashes}                           & 39                                                                   & 45                                                                     \\ \hline
\multicolumn{1}{|l|}{\textit{\textbf{Second dataset cycle}}}                    & \multicolumn{1}{l|}{App crashed}                             & 12                                                                   & 23                                                                     \\ \cline{1-1}
\multicolumn{1}{l|}{}                                                           & \multicolumn{1}{l|}{Unique crashes}                          & 21                                                                   & 28                                                                     \\
\multicolumn{1}{l|}{}                                                           & \multicolumn{1}{l|}{Total crashes}                           & 46                                                                   & 57                                                                     \\ \hline
\multicolumn{1}{|l|}{\textit{\textbf{Third dataset cycle}}}                     & \multicolumn{1}{l|}{App crashed}                             & 23                                                                   & 21                                                                     \\ \cline{1-1}
\multicolumn{1}{l|}{}                                                           & \multicolumn{1}{l|}{Unique crashes}                          & 13                                                                  & 20                                                                     \\
\multicolumn{1}{l|}{}                                                           & \multicolumn{1}{l|}{Total crashes}                           & 87                                                                   & 54                                                                     \\ \cline{2-4} 
\end{tabular}
\end{table}

\section{Linking approach accuracy}
\RQ{2} \textit{To what extent can we link the defects arose from automated testing tools?}
\smallbreak
To answer the \RQ{2}, we exploited the linking procedure provided by \toolname. 
Concretely, we selected from the previously tested dataset the 3 apps which seemed most relevant to our study with enough unique crash reports generated by \monkey and \sapienz and a set of adequate reviews. 
These are \textit{com.amaze.filemanager}, \textit{com.danvelazsco.fbwrapper} and \textit{com.ringdroid}.
We limited our study to a small group of apps because the time constraints and the high effort to \emph{manually validate the data. 
\begin{table}[tb]
	\centering
%	\caption{Precision of the Linking and Complementarity of Reviews and Stack Traces}
	\caption{Precision of the Linking procedure}
%		\vspace{-2mm}
	\label{tbl: precision}
	\begin{tabular}{ccccc}
		\toprule
		\textbf{App} & \textbf{Precision}  \\ 
		\midrule
		\textit{com.amaze.filemanager} & 60\% \\
		\textit{com.danvelazsco.fbwrapper} & 72\% \\
		\textit{com.ringdroid} & 64\% & \\
		\hline
		\textbf{Average} & \textbf{65\%} \\
		\bottomrule
	\end{tabular}
	\vspace{-2.5mm}
\end{table}}
Table \ref{tbl: precision} shows the precision values obtained by the linking approach implemented by \toolname\ over our limited dataset. 
The results reported in this table confirm the accuracy and reliability of our linking procedure. 
Indeed, it shows its effectiveness in achieving quite high precision scores, \ie the percentage of correctly retrieved links was \textbf{65\%}.
We can affirm, that our approach performs in an accurate manner, providing a precise correlation between stack traces and crash-related user feedbacks. 

\vspace{1em}
\smallbreak
\RQ{3} \textit{How complementary are the two source of information? Can we leverage on both of them to increase the effectiveness of the testing process?}
\smallbreak
To answer \RQ{3}, we firstly identified those links which correctly correlated a stack trace with its correspondent user review.  
Secondly, we categorized different type of issues concerning the crash-related user reviews and the stack traces, computing the followings metrics: 
\begin{itemize}
	\item $I_C$: \% of issues reported in both reviews and crash logs;
	
	\item $I_R$: \% of issues reported only in user reviews; 
	 %\ie the number of ;
	
	\item $I_T$: \% of issues reported only in crash logs. 
\end{itemize}
In order to compute these metrics, we first defined $T_{issues}$ as the total number of issues
discovered for a given app.
Therefore, consider $L_{pos}$ as the number of unique true positive links between crash reports and crash-related user feedbacks revealed by \toolname. 
Afterwards, let be $C_{logs}$ the number of crash reports which have been extracted but remain unlinked to any review.
Then, consider $R_{crash}$ as the number of user reviews, for which there exists no correlation to any crash report. 
Finally, we can define the relation between the three above proposed metrics as follows: 
$$T_{issues} = L_{pos} + C_{logs} + R_{crash} $$
%
Thus, we can formally introduce the three overlap metrics introduced above as follow: 
	\begin{equation*}
	I_C = {L_{pos}\over T_{issues}}
	\qquad
	I_R = {R_{crash}\over T_{issues}}
	\qquad
	I_T = {C_{logs}\over T_{issues}}
	\end{equation*}
	


\begin{table}[tb]
	\centering
%	\caption{Precision of the Linking and Complementarity of Reviews and Stack Traces}
	\caption{Complementarity of Reviews and Stack Traces}
%		\vspace{-2mm}
	\label{tbl: metrics}
	\begin{tabular}{ccccc}
		\toprule
		\textbf{App} & $\mathbf{I_C}$ & $\mathbf{I_R}$ & $\mathbf{I_T}$ \\ 
		\midrule
		\textit{com.amaze.filemanager} & 17\% & 50\% & 33\% \\
		\textit{com.danvelazsco.fbwrapper} & 45\% & 45\% & 10\% \\
		\textit{com.ringdroid} & 24\%	 & 62\% & 14\%\\
		\hline
		\textbf{Average} & \textbf{29\%}& \textbf{52\%}& \textbf{19\%}\\
		\bottomrule
	\end{tabular}
	\vspace{-2.5mm}
\end{table}


Table \ref{tbl: metrics} shows the achieved results for our reduced dataset. 
As we can see from the it, the results concerning the $I_R$ values (\ie the percentage of issues we can only find in user reviews) show in average greater scores than the $I_T$ values (\ie the percentage of issue encountered uniquely through automated tools).
Indeed, the number of issues reported only in user reviews is, on average, 52\%. This percentage is conspicuously greater than the number expressing the other two metrics (in fact, $I_C$ shows 29\% and $I_C$ only 19\%). 
From these results, we can draw the following conclusion: 
\smallbreak
\begin{tcolorbox}[colback=white, size=small]
{\small
\textbf{Finding 1:}   The number of issues highlighted 
 by user reviews is substantially greater that the number of issues raised from the stack traces obtained through automated tools.}
 \end{tcolorbox}
%\begin{tcolorbox}[colback=white, size=small]
%{\small



As highlighted in the table \ref{tbl: metrics}, the number of common issues detected when relying on both user reviews and stack traces is, on average, only 29\%. 
To find the cause of a such low percentage, we manually analysed the user reviews of our reduced dataset. 
Indeed, we found that the content of the reviews tend to describe the scenario of a crash, occurring during a particular input event.
However, such scenario may imply some sensible sequences of events, which are hard to randomly replicate by automated testing tools. 
For instance, for the \texttt{com.danvelazsco.fbwrapper} app, the review shown below claims about a crash occurring during a particular swipe input event, which is probably hardly replicable by automated testing tools. 
\smallbreak
\emph{\small``Quick fix for messages crash Slide in from the right go to preferences and use either desktop version or basic version...''}
\smallbreak
Similarly, for the \texttt{com.ringdroid} app an user claim that:
\smallbreak
\emph{\small``Force closes when I search Maybe the problem is my large library but it truly is unusable...''}
\smallbreak
In this case, the user imputes the crash to his large library: obviously such particular conditions are difficult to be reproduced with testing tools.
Thus, we can draw our second finding: 




\smallbreak
\begin{tcolorbox}[colback=white, size=small]
{\small 
\textbf{Finding 2:}  Issues identified by \toolname\ in user feedback, tend to be conceptually different, but also complementary to the issues detected by automated testing tools.}
\end{tcolorbox}

With our finding we believe that we confirmed the importance and relevance to integrate user reviews in the validation process of a mobile application. 
Our results are quite promising, since they concretely showed the complementary between both source of information. 
However, there are still some limitations in the automated testing tools, since in many cases they are not able to reproduce some sensible scenarios describe in the user feedback. 
We argue that, we might augment the information in stack traces with the aim to link more crash-related user reviews with their correspondent crash reports. 


