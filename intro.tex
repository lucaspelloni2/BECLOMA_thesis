% !TEX root =  ./lucas_thesis.tex
%------------------------- THESIS OVERVIEW ------------------------
% todo: set label for chapters
This thesis is placed in the context on mobile automated testing. In particular, we aim to shed some initial lights into the possibility to integrate user feedback into such process, in order to increase its efficiency and efficacy.
The first chapter of this work gives an overview about the context in which the thesis is placed, as well the motivations behind that led to such thesis. Follows a brief introduction about the mobile testing, its limitations and challenges. At the end, we disclose the research questions leading our investigations. 
The remainder of this thesis is organized as follow. 

Chapter 2 presents the recent literature in two areas of interest for this work, \ie the automated mobile testing and the exploiting of mobile stores mining in order to facilitate the software maintenance activities.

Chapter 3 describes the approach we developed in order to investigate our main goal.

Chapter 4 presents \toolname, a tool that firstly relies on \monkey and \textbf{Sapienz} in order to test a set of Android applications, collecting crash logs (\ie stack traces). Secondly, exploiting classic Information Retrieval (IR) techniques, the tool is able to cluster the logs that refers to the same bugs. Finally, it relies on a novel approach for liking such clusters of crash logs with the users reviews that refers to the bug revealed by them. 
%The tool enables developers to link natural language contained inside user feedbacks and pieces of source code, such as names of classes, methods, functions, etc. 

Chapter 5 describes the empirical study we conducted in order to evaluate the aforementioned approach on a set of 3 mobile apps (available on \textit{FDroid \footnote{\url{https://f-droid.org}}}) with their correspondent reviews crawled for the Google Play Store\footnote{\url{https://play.google.com/}}. 

Finally, Chapter 6 closes the main body of the thesis drawing the necessary conclusions and spreading the main ideas for future contributions.


%------------------------- CONTEXT: TESTING IN GENERAL -------------------
\section{Context}
Testing is the action of inspecting the behavior of a program, with the intention of finding anomalies or errors \cite{testing}.
The goal behind software testing is exercise the system under test (SUT) in order the hopefully reveal as many bugs as possible. In order to do that, it aims to reach the the highest \textit{test coverage} with the smallest number of \textit{test cases}. A test case can be viewed as a set of program inputs, each of them gets associated with an expected result (\ie \textit{oracle}) when they are executed. 

Nowadays, software testing is widely recognized as an essential part of any software development process, presenting however an extremely expensive activity. Indeed, test all combinations of all possible input values classes for an application \cite{glinz} requires a lot of workforce and it is almost always unthinkable to reach a testing-coverage of 100\%. For such reason, techniques like Search-Based approaches have been successfully exploited in order to automate the test data generation in the last years \cite{Harman_et_al_2015}.
Furthermore, testing is often performed under time and budget constraints \cite{grano} and complex applications might have a very large number of potential test scenarios, many of which could be really difficult to predict. Indeed, \textit{Dijkstra} \cite{dijkstra} observed that testing a software does not imply a demonstration of the right behavior of the program, but it only aims to demonstrate the presence of faults, not their absence. 
%But then, why try to test a program even knowing that a full coverage (and so a complete validation of the software) cannot be reached? Well, the answer is quite simple. 
So, deeply test a SUT and increases the probability that this software would behave as expected also in the untested cases \cite{glinz}. 

In general, there are four testing levels a tester should perform in order to investigate the behavior of a traditional software: 
\begin{enumerate}
\item Unit testing; 
\item Integration testing; 
\item System testing; 
\item Acceptance testing.
\end{enumerate}

With \textit{unit testing}, the application components are viewed and split into  individual \textit{units} of source code, which are normally functions or small methods. Intuitively, one can view a unit as the smallest testable part of an application. This kind of testing is usually associated with a \textit{white-box} approach. \textit{Integration testing} is the activity of finding faults after testing the previous individually tested units combined and tested as a group together. \textit{System testing} is conducted on a complete, integrated system to evaluate the compliance with its requirements. It can be imagined as the last checkpoint before the end customer. Instead, \textit{acceptance testing} (or \textit{customer testing}) is the last level of the testing process, which states whether the application meets the user needs and whether the implemented system works for the user. This kind of testing is usually associated with a \textit{black-box} approach. 
\\
These mentioned testing levels shall be sequentially executed and are driven by two testing methodologies \cite{white-box, black-box}: 
\begin{itemize}
\item black-box testing;
\item white-box testing.
\end{itemize} 
With \textit{black-box} testing (also called functional testing) the tester does not have any prior knowledge of the internal structure of the SUT. He tests only the functionalities provided by the software without any access to the source code. Typically, when performing a black box test, a tester interacts with the system's user interface by providing inputs, examining the compliance of the output with the oracles. On the other side, with \textit{white-box} testing, also \textit{glass testing} or \textit{open box testing} \cite{grano}, the test cases are extrapolated from the internal software's structure. Indeed, the tester writes the test cases that reflect paths through the code, with the aim usually to maximize a coverage criterion. \\
%todo: check the part below. Not fully convincing 
In practice, the whole testing process is strictly structured and it gets implemented through the use of different testing models. Usually, the selection of one model rather than another has a huge impact on the final testing result is carried out. The most popular and applied testing process models in practice are the \textit{V-model} \cite{vmodel}, the \textit{Waterfall model}\cite{waterfallmodel} and the \textit{Spiral model} \cite{spiralmodel}. \\
As shown, there exist few testing steps, methodologies and different models to accompany a testing process of a traditional software. However, when a tester is intent to define and choose the right pieces for testing a software in a careful manner, the common goal behind the testing stays always the same, i.e. generate a \textit{test suite} (a set of \textit{test cases}), with the smallest number of test cases causing the largest number of failures in the system \cite{grano}, in order to increase the reliability of the system. 
However as mentioned before, the process of finding the correct test scenarios, afterwards execute them and finally report their results and compare them to the previously written specifications, might be time-consuming and cost-intensive. In fact, testing costs have been estimated at being at least half of the entire development cost \cite{Beizer:1990:STT:79060}. For this reason, it is necessary to reduce them, trying to improve the effectiveness of the whole testing process, with the aim to automate it. 

\section{Motivation}
%------------- PASSAGGIO DAL TESTING TRADIZIONALE AL MOBILE -------------
In the last years, we witnessed a huge growth in the usage of mobile software, due to the advent of the \textit{mobile age}. Indeed, nowadays, application running on mobile devices are becoming so widely adopted, so that they have represented a remarkable revolution in the IT sector. In fact, in three years, over 300,000 mobile applications have been developed, \cite{muccini}, 149 billions downloaded only in 2016 \cite{statista} and 12 million mobile app developers (expected to reach up 14 million in 2020) maintaining them \cite{DevRelate}. \\
This majestic revolution in the IT sector has had also a huge impact on the software testing area. This because, mobile applications differ from traditional software and so differ also their testing techniques. 
In fact, mobile applications are structured around Graphical User Interface (GUI) events and activities, thus exposing them to new kinds of bugs and leading to a higher defect density compared to traditional desktop and server applications \cite{Hu:2011:AGT:1982595.1982612}. Furthermore, they are context-aware \cite{muccini}. 
Therefore, each mobile application is aware of the environment in which it is running and provides inputs according to its current context. This has some implications for the testing, because a test case running on a specific context may lead to its expected results, while the same test case running on another environment may be error-prone. In fact, bugs related to contextual inputs are quite frequent \cite{muccini}. \\
For this reason, mobile applications require new specialized and different testing techniques \cite{muccini} in order to ensure their reliability. In this sense, an extremely valid approach has been the GUI testing. In particular, in this kind of testing, each test case is designed and run in the form of sequences of GUI interaction events, with the aim to state whether an application meets its written requirements. 

% ------------- LE SFIDE DEL MOBILE TESTING E AUTOMATED GUI TESTING -----------------
As traditional testing, GUI testing can be performed either manual or automatic. However, a manual approach would require a lot of programming and may be time-consuming. \\  
For this purpose, with the aim to support developers in building high-quality applications, the research community has recently developed novel techniques and tools to automate their testing process \cite{sapienz, dynodroid ,muccini,Hu:2011:AGT:1982595.1982612}. \\
%The most famous automated GUI testing tools and their properties are discussed in the chapter \textit{Related Work}. \\
These techniques consist of automatically create test cases through the generation of UI and system events, which are transmitted to the application under test, with the aim to cause some crashes. Afterwards, if an exception occurred, these tools usually save in log files the correspondent stack traces, along with the sequence of events that led to the exceptions \cite{muccini}. 
However, despite a strong evidence for automated testing approaches in verifying GUI application and revealing bugs, these state-of-art tools cannot always achieve a high code coverage \cite{Nagappan2015}. 
One reason is that an automated event-test-generation tool is not suited for generating inputs that require human intelligence (e.g. "\textit{inputs to text boxes that expect valid passwords, or playing and winning a game with a strategy"} \cite{dynodroid}, etc.). This because, current solutions generate redundant/random inputs that are insufficient to properly simulate the human behaviour, thus leaving feature and crash bugs undetected until they are encountered by the users. Furthermore, the crash reports that they generate usually lack of contextual information and are difficult to understand and analyze \cite{Chen, Joorabchi}. 
For such reasons, sometimes a time-consuming manual approach can be needed for testing an application \cite{Nagappan2015}. \\

% ------------- REVIEWS PER AIUTARE GLI SVILUPPATORI NELLA FASE DI TESTING -----------------
However, GUI testing could not be the only approach to help developers find bugs in a mobile application. Nowadays, the exponential growth of the mobile stores offers an enormous amount of informations and feedbacks from users. In fact, these users feedback are playing more and more a paramount role in the development and maintenance of mobile applications. Therefore, another different strategy for giving further support to developers during the testing phase of their applications is to incorporate opinions and reviews of the end-users during the software's evolution process. \newline
Indeed, information contained within \textit{user review}s can be exploited to overcome the limitations of state-of-the-art tools for automated testing of mobile apps in the following ways \cite{cristal}: 
\begin{enumerate}
\item they can complement the capabilities of automated testing tools, by identifying bugs that they cannot reveal; 
\item they can facilitate the diagnosis and reproducibility of bugs (or crashes), since user reviews often describe the actions that led to a failure; 
\item they can be used to prioritize bug resolution activities based on the users' requests, possibly helping developers in increasing the ratings of their apps.
\end{enumerate} 

\section{Motivation Example}
Following, I provide a concrete example of how we plan to compare and analyze the two sources of information: user reviews and reports generated by automated mobile applications testing tools. Given app reviews, we first plan to automatically select the ones describing crashes and bugs in features or in UI elements.
%\textit{Feature \& UI Bugs} and \textit{Crashes}. 
One such example is the following: 
\smallbreak
\emph{\small``Barely works I only installed this for the android sharing function but it usually loads a white screen when I try to use this feature. It's no better than using a browser to view Facebook otherwise.''}. 
\smallbreak
The user complains about the app sharing function that shows a white screen when activated. A report generated by one of available automated testing tools, \textsc{Sapienz} \cite{sapienz}, includes the stack trace below, where \textsc{Sapienz} is able to find a \texttt{NullPointerException} while exercising the sharing functionality. 
\begin{lstlisting}[basicstyle=\fontsize{6}{8}\ttfamily]
java.lang.NullPointerException
  at com.danvelazco.fbwrapper.activity.BaseFacebookWebViewActivity.shareCurrentPage(BaseFacebookWebViewActivity.java:418)
  at com.danvelazco.fbwrapper.FbWrapper.access$700(FbWrapper.java:26)
  at com.danvelazco.fbwrapper.FbWrapper$MenuDrawerButtonListener.onClick(FbWrapper.java:376)
  at android.view.View.performClick(View.java:4438)
  at android.view.View.onKeyUp(View.java:8241)
  ....
  ....
  at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:611)
  at dalvik.system.NativeStart.main(Native Method)
\end{lstlisting}

The aim behind my thesis is to investigate the correlation between these \textit{user reviews} and the \textit{crash logs} generated from the automated state-of-art tools described in the previous sections. 
I combine these two sources of information implementing a linking procedure to add contextual details to the logs generated by automated mobile testing tools. I argue that this approach can be fruitfully used to support developers in determining which bugs should be addressed first, prioritizing the arose failures. It is worth to notice that such prioritization is directly guided by the user feedback with the aim to maximize their satisfaction and consequentially, the app rating.
I argue that this investigation might improve the automated testing process and might help other developers in validating their mobile applications, using \toolname, the implemented tool described in the \textit{Chapter 4}.



\section{Research questions}
In this thesis, I conduct a preliminary study to investigate the first and most important point from the previous list: \textit{the complementarity between user feedback and the outcomes of automated testing tools}. This research represents a first fundamental step toward the integration of user feedback into the testing process of mobile apps. Thus, I conducted my thesis to answer the following research questions:

\begin{itemize}
 %\vspace{-1.5mm}
	\item \RQ{1}: \textit{Which are the most important tools for automatic testing of Android applications?} %I want to investigate 
	
	\item \RQ{2}: \textit{To what extent can we link the defects arose from automated testing tools with user reviews?}  
	
	\item \RQ{3}: \textit{What kind of defects are detected by state-of-art tools proposed for GUI testing? How these defects are effectively perceived by users?} 

\end{itemize}
For answering the \RQ{1}, I want to investigate which are the most important automated testing tools used for testing Android mobile applications. For this purpose, I present in the Chapter ~\ref{sec:related} the main research in this field, giving an accurate overview about the testing tools in general, their pros and cons and their exploration strategies. 

For the \RQ{2}, I implement a linking procedure which uses a experimental-based threshold in order to state whether a review refers to a specific \textit{crash log}. Based on the result of this linking approach I am able to discuss and answer the second research question. 

For the \RQ{3}, TODO!
