% !TEX root =  ./lucas_thesis.tex
%------------------------- THESIS OVERVIEW ------------------------
\label{chapter:intro}
This thesis is placed in the context on mobile automated testing. In particular, we aim to shed some initial lights into the possibility to integrate user feedback into such process, in order to increase its efficiency and efficacy.
The first chapter of this work gives an overview about the context in which the thesis is placed, as well the motivations behind that led to such thesis. Follows a brief introduction about the mobile testing, its limitations and challenges. At the end, we disclose the research questions leading our investigations. 
The remainder of this thesis is organized as follow. 

Chapter \ref{chapter:related} presents the recent literature in two areas of interest for this work, \ie the automated mobile testing and the exploiting of mobile stores mining in order to facilitate the software maintenance activities.

Chapter \ref{chapter:approach} describes the approach we developed in order to investigate our main goal.

Chapter \ref{chapter:tool} presents \toolname, a tool that firstly relies on \monkey and \sapienz in order to test a set of Android applications, collecting crash logs (\ie stack traces). Secondly, exploiting classic Information Retrieval (IR) techniques, the tool is able to cluster the logs that refer to the same bugs. Finally, it relies on a novel approach for linking such clusters of crash logs with the users reviews that refer to the bug revealed by them. 
%The tool enables developers to link natural language contained inside user feedbacks and pieces of source code, such as names of classes, methods, functions, etc. 

Chapter \ref{chapter:results} describes the empirical study we conducted in order to evaluate the aforementioned approach on a set of 3 mobile apps (available on \textit{FDroid \footnote{\url{https://f-droid.org}}}) with their correspondent reviews crawled from the Google Play Store\footnote{\url{https://play.google.com/}}. 

Finally, Chapter \ref{chapter:conclusion} closes the main body of the thesis drawing the necessary conclusions and spreading the main ideas for future contributions.


%------------------------- CONTEXT: TESTING IN GENERAL -------------------
\section{Context}
Testing is the action of inspecting the behavior of a program, with the intention of finding anomalies or errors \cite{testing}.
The goal behind software testing is to exercise the system under test (SUT) in order to hopefully reveal as many bugs as possible. In order to do that, it aims to reach the highest \textit{test coverage} with the smallest number of \textit{test cases}. A test case can be viewed as a set of program inputs, each of them gets associated with an expected result (\ie \textit{oracle}) when they are executed. 

Nowadays, software testing is widely recognized as an essential part of any software development process, presenting however an extremely expensive activity. Indeed, test all combinations of all possible input values classes for an application \cite{glinz} requires a lot of workforce and it is almost always unthinkable to reach a testing-coverage of 100\%. 
Furthermore, testing is often performed under time and budget constraints \cite{grano} and complex applications might have a very large number of potential test scenarios, many of which could be really difficult to predict. Indeed, \textit{Dijkstra} \cite{dijkstra} observed that testing a software does not imply a demonstration of the right behaviour of the program, but it only aims to demonstrate the presence of faults, not their absence. 
%But then, why try to test a program even knowing that a full coverage (and so a complete validation of the software) cannot be reached? Well, the answer is quite simple. 
So, deeply test a SUT increases the probability that this software would behave as expected also in the untested cases \cite{glinz}. 

In general, there are four testing levels a tester should perform in order to investigate the behaviour of a traditional software: 
\begin{enumerate}
\item Unit testing; 
\item Integration testing; 
\item System testing; 
\item Acceptance testing.
\end{enumerate}

With \textit{unit testing}, the application components are viewed and split into  individual \textit{units} of source code, which are normally functions or small methods. Intuitively, one can view a unit as the smallest testable part of an application. This kind of testing is usually associated with a \textit{white-box} approach. \textit{Integration testing} is the activity of finding faults after testing the previous individually tested units combined and tested as a group together. \textit{System testing} is conducted on a complete, integrated system to evaluate the compliance with its requirements. It can be imagined as the last checkpoint before the end customer. Instead, \textit{acceptance testing} (or \textit{customer testing}) is the last level of the testing process, which states whether the application meets the user needs and whether the implemented system works for the user. This kind of testing is usually associated with a \textit{black-box} approach. 
\\
These mentioned testing levels shall be sequentially executed and are driven by two testing methodologies \cite{white-box, black-box}: 
\begin{itemize}
\item black-box testing;
\item white-box testing.
\end{itemize} 
With \textit{black-box} testing (also called functional testing) the tester does not have any prior knowledge of the internal structure of the SUT. He tests only the functionalities provided by the software without any access to the source code. Typically, when performing a black box test, a tester interacts with the system's user interface by providing inputs, examining the compliance of the output with the oracles. On the other side, with \textit{white-box} testing (also \textit{glass testing} or \textit{open box testing}), the test cases are extrapolated from the internal software's structure \cite{grano}. Indeed, the tester writes the test cases that reflect paths through the code, with the aim usually to maximize a coverage criterion. 
At the end, those techniques aim to generate a \textit{test suite} composed by a set of \textit{test cases}, \ie a set of conditions under which a tester will determine whether a software system is working as it was described by its original specification.

As previous explained, testing can be done through different approaches, methodologies and level of details. However, the common goal behind the testing remains the same, i.e. generate the smaller test suite able to reveal the largest number of failures, in order to increase the reliability of the system \cite{grano}. 
Nevertheless, the process of finding the correct test scenarios, generate the consequential data inputs, execute and finally report the results, comparing them to the previously written specifications, might be time-consuming and cost-intensive. In fact, testing costs have been estimated at being at least half of the entire development cost \cite{Beizer:1990:STT:79060}. For this reason, it is necessary to reduce such costs automatizing the various process occurring in testing, trying to improve the effectiveness of the whole process. In such described scenario, for instance, a remarkable example is the application of Search-Based approaches that have been successfully exploited in last years in order to automate the test data generation \cite{Harman_et_al_2015}.

\section{Motivation}
%------------- PASSAGGIO DAL TESTING TRADIZIONALE AL MOBILE -------------
In the last years, we witnessed a huge growth in the usage of mobile software, due to the advent of the \textit{mobile age}. Indeed, nowadays, application running on mobile devices are becoming so widely adopted, so that they have represented a remarkable revolution in the IT sector. In fact, in three years, over 300,000 mobile applications have been developed \cite{muccini}, 149 billions downloaded only in 2016 \cite{statista} and 12 million mobile app developers (expected to reach up 14 million in 2020) maintaining them \cite{DevRelate}. 

Because of the importance of the mobile development field nowadays, developers aim to sustain the competition in acquire and retain users increasing the quality of their application. Playing the testing a crucial role in achieving such quality, several approaches for automated mobile testing have been recently developed. This because, mobile applications differ from traditional software and so there is also a need for developing new ad-hoc testing techniques. 
In fact, mobile applications are structured around Graphical User Interface (GUI) events and activities, thus exposing them to new kinds of bugs and leading to a higher defect density compared to traditional desktop and server applications \cite{Hu:2011:AGT:1982595.1982612}. Furthermore, they are context-aware \cite{muccini}. 
This means that each mobile application is aware of the environment in which it is running and exposes a different behavior according to its current context. This has some implications for the testing, because a test case running on a specific context may lead to different results. In fact, bugs related to contextual inputs are quite frequent \cite{muccini}. 

For this reason, mobile applications require new specialized and different testing techniques in order to ensure their reliability \cite{muccini}. Being mobile applications event-based systems used through UI interaction, the GUI testing needs to have a prominent role in app testing. In particular, in this kind of testing, each test case is designed and run in the form of sequences of GUI interaction events, with the aim to state whether an application meets its written requirements. 

% ------------- LE SFIDE DEL MOBILE TESTING E AUTOMATED GUI TESTING -----------------
As traditional testing, GUI testing can be performed either manual or automatic. However, a manual approach would require a lot of programming and may be time-consuming. 
For this purpose, with the aim to support developers in building high-quality applications, the research community has recently developed novel techniques and tools to automate their testing process \cite{sapienz, dynodroid ,muccini,Hu:2011:AGT:1982595.1982612}. 
%The most famous automated GUI testing tools and their properties are discussed in the chapter \textit{Related Work}. \\
These techniques consist of automatically create test cases through the generation of UI and system events, which are transmitted to the application under test, with the aim to cause some crashes. Afterwards, if an exception occurred, these tools usually save in log files the correspondent stack traces, along with the sequence of events that led to the exceptions \cite{muccini}. 
However, despite a strong evidence for automated testing approaches in verifying GUI application and revealing bugs, these state-of-art tools cannot always achieve a high code coverage \cite{Nagappan2015}. 
One reason is that current approaches are not suited for generating inputs that require human intelligence (e.g. \textit{"inputs to text boxes that expect valid passwords, or playing and winning a game with a strategy"}) \cite{dynodroid}. This inability to replicate the human behaviour, often leaving feature and crash bugs undetected until they are encountered by the users.
Furthermore, current solutions generate redundant/random causing incredibly long sequences of events needed to replicate a crash, while most of such inputs could be avoided. 
In addition, the crash reports that they generate usually lack of contextual information and are difficult to understand and analyze \cite{Chen, Joorabchi}. 
For such reasons, sometimes a time-consuming manual intervention may be needed for testing appropriately an application \cite{Nagappan2015}. 

% ------------- REVIEWS PER AIUTARE GLI SVILUPPATORI NELLA FASE DI TESTING -----------------
Despite this limitations and challenges, the mobile developing environment has the huge advantage, compared to the traditional one, to have a huge amount of information available from mobile stores, which have been introduced as a new selling paradigm for mobile applications. Indeed, the exponential growth of the mobile stores offers an enormous amount of information and feedbacks from users that researchers and partitioner can use to develop better applications. In fact, these users feedback are playing more and more a paramount role in the development and maintenance of mobile applications. Indeed, recent research focused on how such knowledge might be successful exploited (Section \ref{section:review_usage}). 
We argue that also the GUI testing could take advantage in some way of these information. That is why, in this thesis work we shed some initial lights into the integration of feedbacks extracted from user reviews in the automated testing for mobile applications.
In detail, we hypothesize that such integration can be exploited to overcome the aforementioned limitations of state-of-the-art tools for automated testing of mobile apps in the following ways \cite{cristal}: 
\begin{enumerate}
\item it can complement the capabilities of automated testing tools, by identifying bugs that they cannot reveal; 
\item it can facilitate the diagnosis and reproducibility of bugs (or crashes), since user reviews often describe the actions that led to a failure; 
\item it can be used to prioritize bug resolution activities based on the users' requests, possibly helping developers in increasing the ratings of their apps.
\end{enumerate} 
In particular, in this work we investigated this first point by developing an approach able to link the user reviews with the stack traces that refer to the same bug. Furthermore, we conducted a preliminary study to understand how complementary are the two sources of information and in particular whether we can use the reviews to identify some problems that the tools miss. 

\section{Motivation Example}
Following, we provide a concrete example of how we plan to compare and analyse the two sources of information: user reviews and reports generated by automated mobile applications testing tools. Given app reviews, we first plan to automatically select the ones describing crashes and bugs in features or in UI elements.
%\textit{Feature \& UI Bugs} and \textit{Crashes}. 
A good example of useful review is the following: 
\smallbreak
\emph{\small``Barely works I only installed this for the android sharing function but it usually loads a white screen when I try to use this feature. It's no better than using a browser to view Facebook otherwise.''}. 
\smallbreak
The user here complains about the app sharing function that shows a white screen when activated. A report generated by one of available automated testing tools, \textsc{Sapienz} \cite{sapienz}, includes the stack trace below, where \sapienz is able to find a \texttt{NullPointerException} while exercising the sharing functionality. 
\smallbreak
\begin{lstlisting}[basicstyle=\fontsize{5.5}{8}\ttfamily]
java.lang.NullPointerException
  at com.danvelazco.fbwrapper.activity.BaseFacebookWebViewActivity.shareCurrentPage(BaseFacebookWebViewActivity.java:418)
  at com.danvelazco.fbwrapper.FbWrapper.access$700(FbWrapper.java:26)
  at com.danvelazco.fbwrapper.FbWrapper$MenuDrawerButtonListener.onClick(FbWrapper.java:376)
  at android.view.View.performClick(View.java:4438)
  at android.view.View.onKeyUp(View.java:8241)
  ....
  ....
  at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:611)
  at dalvik.system.NativeStart.main(Native Method)
\end{lstlisting}
\smallbreak
So, the aim behind this thesis is to investigate the complementarity between the user reviews and the crash reports generated by the automated state-of-art tools described in Section \ref{sec:tools}.
The preliminary step into this analysis it to implement a linking procedure of the two sources of information. We argue that this approach can be fruitfully used to support developers in determining which bugs should be addressed first, prioritizing the arose failures. It is worth to notice that such prioritization is directly guided by the users feedback with the aim to maximize their satisfaction and consequentially, the app rating.
We argue that this investigation might improve the automated testing process and might help other developers in validating their mobile applications, using \toolname, the implemented tool described in the Chapter \ref{chapter:tool}.

\section{Research questions}
\label{section:rqs}
In this thesis, we conduct a preliminary study to investigate the first and most important point from the previous list, \ie \textit{the complementarity between user feedback and the outcomes of automated testing tools}. This research represents a first fundamental step toward the integration of user feedback into the testing process of mobile apps. Thus, we conducted my thesis aiming to answer the following research questions:

\begin{itemize}
	\item \RQ{1}: \textit{Which are the most important tools for automatic testing of Android applications?} %I want to investigate 
	\item \RQ{2}: \textit{To what extent can we link the defects arose from automated testing tools with user reviews?}  
%	\item \RQ{3}: \textit{What kind of defects are detected by state-of-art tools proposed for GUI testing? How these defects are effectively perceived by users?} 
	\item \RQ{3}: \textit{How complementary are the two source of information? Can we leverage on both of them to increase the effectiveness of the testing process?}
\end{itemize}
The first research question is merely represented by a literature overview to identify which are the state-of-art tools in automated Android testing. To answer such question we presented the main research in this field in Chapter \ref{sec:related}, giving an accurate overview about the testing tools in general, their pros and cons and their different exploration strategies. 

To answer the \RQ{2}, we implement a linking procedure which relies on Information Retrieval (IR) techniques and uses an experimental-based threshold in order to state whether a review refers to a specific stack trace. We described the implemented approach in Chapter \ref{chapter:approach}, discussing its precision in Chapter \ref{chapter:results}.
Finally, for our last research question we relied on the aforementioned implemented linking technique to understand and identify whether particular kind of bugs cannot be revealed by current testing tools but can be observed in user reviews (and vice versa). Again, we describe the achieved results in Chapter \ref{chapter:results}.